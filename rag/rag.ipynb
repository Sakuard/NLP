{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f0d7d3-20e5-4739-b966-d53c7d024adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import asyncio\n",
    "import ollama\n",
    "import os\n",
    "import uuid\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335d2b16-05be-4942-a82a-d7cf0bcb8325",
   "metadata": {},
   "source": [
    "## This block is for \"global-variable\" setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e010d21-ad31-43e6-a167-128585536b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_size = 512\n",
    "shift_size = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39186f45-96ed-436a-85f1-f0b8d738be2c",
   "metadata": {},
   "source": [
    "## This block is for \"ChromaDB\" setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0553ada-a8c3-4533-9d26-72e54aae65dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = \"docs\"\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "collection = chroma_client.get_or_create_collection(name=collection)\n",
    "\n",
    "def heartbeat():\n",
    "    return chroma_client.heartbeat()\n",
    "\n",
    "def collection_add(i, embedding, document, metadata):\n",
    "    collection.add(\n",
    "        ids=[i],\n",
    "        embeddings=[embedding],\n",
    "        documents=[document],\n",
    "        metadatas=[metadata]\n",
    "    )\n",
    "\n",
    "def collection_query(query, n_results):\n",
    "    results = collection.query(\n",
    "        query_embeddings=query,\n",
    "        n_results=n_results\n",
    "    )\n",
    "    return results\n",
    "\n",
    "def collection_reset():\n",
    "    while True:\n",
    "        data_ids = collection.peek()['ids']\n",
    "        if not data_ids:\n",
    "            break\n",
    "        for i in data_ids:\n",
    "            # print(i)\n",
    "            collection.delete(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79f64fc-f43c-4c07-83db-d99df1d54629",
   "metadata": {},
   "source": [
    "## This block is for \"DB like controller\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "527f95a6-5986-4ea8-9ebb-017d157b515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_status(filename):\n",
    "    # 檢查文件是否存在且不為空\n",
    "    if not os.path.exists(filename) or os.stat(filename).st_size == 0:\n",
    "        return {}\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            return json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            # 如果 json 文件內容格式錯誤，返回空字典\n",
    "            return {}\n",
    "\n",
    "def save_status(filename, data):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "sql_file = 'sql.json'\n",
    "sql_data = load_status(sql_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dec0c0-29ed-4635-990e-87eecd042b31",
   "metadata": {},
   "source": [
    "## This block is for \"RAG pipeline related\" funciton\n",
    "- prompt_expansion: use LLM to gen more related prompt to evaluate the response precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4054423c-d6bb-4dbb-8f9a-42038bedbcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM prompt expansion >> 類似使用 \"many-hit\" 的方式來提高搜尋命中率\n",
    "def prompt_expansion(query):\n",
    "    expansive_prompts = ollama.generate(\n",
    "        model=\"llama3\",\n",
    "        prompt=f\"請用繁體中文，以這個 prompt: '{query}' 為基礎，以 array list 的方式，產生出額外 5 個相關的提問 prompt，我需要的 response 格式為 ['第一個產生的相似提問', '第二個產生的相似提問', '第三個產生的相似提問'] 這樣即可，不需要額外的其他內容，注意，相似提問的內容請以繁體中文呈現為主\"\n",
    "    )\n",
    "    return expansive_prompts['response']\n",
    "\n",
    "def query_rerank(embedding, topk):\n",
    "    results = collection_query(embedding, topk)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e8d19-01af-42b6-826d-03426c3cb308",
   "metadata": {},
   "source": [
    "## This block is for Application layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00c1f81f-6877-4dd9-8cab-c849a7ce285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2dataset = './dataset/'\n",
    "\n",
    "# To load docs from directory\n",
    "def read_documents_from_directory(directory):\n",
    "    documents = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "                filenames.append(filename)\n",
    "    return documents, filenames\n",
    "\n",
    "# To do data-embedding\n",
    "def do_dataset_embedding(path2dataset, filenames, documents):\n",
    "    for i, (f, d) in enumerate(zip(filenames, documents)):\n",
    "        if sql_data.get(f, {}).get('status') != 'done':\n",
    "            process_document(f, d)\n",
    "            file_path = os.path.join(path2dataset, f)\n",
    "            sql_data[f] = {'filename': f, 'path': file_path, 'status': 'done'}\n",
    "        else:\n",
    "            print(f\"...Skipping [DONE] - {f}\")\n",
    "\n",
    "documents, filenames = read_documents_from_directory(path2dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8275487-f6cb-4e79-91c1-273ef41d7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"歐洲遊學\"\n",
    "# expansive_prompts = prompt_expansion(query)\n",
    "# print(f\"extend: {expansive_prompts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f229b6a2-554c-494d-8efa-d3875144c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = ollama.embeddings(prompt=query,model=\"llama3\")[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0101431b-2392-4e4b-b2a0-0c218a810457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_query_result: [['意義不太大不知道發論文,所以我很早就知道幹我超討厭我超討厭盧培和所以我從來都不太會真的不太會,就很煩,都是數學然後,因為我覺得研究比較像是單點突破你要把一個細節把它做出一些新的東西可是從工程師的角度,比如說以軟體工程的角度你要去優化一個系統你有非常多選擇那這個就變成說你今天是一個,Solver Engineer是一個total solution,但你做research變成說你在某一個很特別的一個part,你必須要挖得比別人更深所以我沒辦法做應該說,其實我也不太喜歡那請問子緣,你當時是發現自己有這方面的一個天賦或興趣嗎?其實講到這個我就有點有點不好意思因為說真的,我覺得說天賦嗎?我其實沒有很有天賦,我有看過那種真的研究狂人,他是真的就是無時無刻不在做研究然後我覺得我沒有那麼我不排斥,但是我也沒有到真的很狂熱那我覺得真的會想要念博士主要還是就是有我一些想做的事情然後再加上自己其實我會喜歡看著這個東西,然後想要去把它搞懂那這個搞懂的過程,其實就像Ted說的一樣可能像工作你有很多他是一個total solution,那其實研究某方面來說同樣的問題也有很多種不同的解決方式某種程度來說這也是一種total solutio', '但是他會先讓你有一個picture讓你知道說要到那個階段大概要經歷過哪一些事情,然後你可能要做哪些準備真的是去走一條就是有些人已經幫你除過草你可以在那裡可以稍走一點彎路這也是我們創這些做這些內容分享很重要的一個原因大部分的人除非你今天是馬斯克,你要射火箭沒有人射過,不然大部分樓部都有人走過所以如果你有人可以參考那當然是很好的一件事情我們會把子緣的聯絡方式都放在下面大家有興趣可以關注他的內容那我自己覺得大家可以可能每個人都有每個人不同的一個生活的軌跡但你或許沒辦法帶走他每個部分但你可以帶走他的一個態度我覺得子緣他在很多做事的方式大家可以參考因為我們認識很久,我知道他是一個非常努力的人那這種態度是我自己認為你或許沒有人家聰明你或許沒有人家有資源但是你可以比別人做得更努力那你一樣有機會拿到你喜歡的一個成果那我知道就是在這些大家在找這些資料的時候其實你很難去找到好的管道那我剛剛說了如果有錢人走過,你有一些路可以參考那我認為是蠻重要的那最後可以請子緣幫我們介紹你的自媒體你的一些分享內容大家可以怎麼樣找到你我現在自己就是對,有在經營一個Instagram然後主要就是分享三個主題就是歐洲留學,然後個人成長跟博士分享然後我分', '司新的信金流是不是,拿去租學生租別的那我不好說,我不知道,要問他們他們是二房東,所以管理起來比較麻煩我想說這麼捲的是不是不知道也是有可能,但我確定在國外感覺是滿好特別的一些體驗有沒有什麼體驗是你就是比較覺得不適應的,可能跟台灣你過去才發現,這種文化上有衝擊之類的這個首先衝擊就是下次誰在跟我講德國人準時的話你給我試試看第一個就是其實他們的公眾運輸超常出狀況就是你搭一班德國的火車你可以直接誤點兩小時,然後最後那班車還直接不見然後說,不好意思,我們沒辦法然後你只能再搭下一班然後你要再等個一個多小時這個是第一個就是,在歐洲應該說在德國,大家都很常遇到的狀況然後其實連他們這邊的人都很受不了但是就是沒辦法他們就擺爛這樣,然後再來我覺得如果講到文化上,其實我覺得有兩個地方滿值得分享的第一個就是語言語言真的隔閡太大了大家可能會覺得歐洲人,大家可能很會講英文對,他們普遍來說英文能力滿好的可是他們其實真的不太願意講英文而且聽說跟法國人比起來,德國人已經比較願意講而且慕尼亞也是一個國際城市可是,譬如說我在實驗室他們很常會講德文,就會突然一群人就一起,聚在一起,然後講德文然後也不管旁邊的人聽不聽得懂這個有時候其實會還滿讓人滿挫折的吧']]\n",
      "A simple one!\n",
      "\n",
      "My answer is: **YES**\n"
     ]
    }
   ],
   "source": [
    "topk = 3\n",
    "results = collection_query(embedding, topk)\n",
    "# print(results)\n",
    "tmp_query_result = []\n",
    "for d in results['documents']:\n",
    "    tmp_query_result.append(d)\n",
    "\n",
    "print(f\"tmp_query_result: {tmp_query_result}\")\n",
    "query_justify = ollama.generate(\n",
    "    model=\"llama3\",\n",
    "    prompt=f\"It's a YES/No question. 如果下列於 array list [] 內的內容，有任何可以是這個問題: '{query}' 的答案，請回答 yes,否則請回答 not\"\n",
    ")\n",
    "print(query_justify['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d21934-a627-49cf-aefb-181782e7d13a",
   "metadata": {},
   "source": [
    "## This block is testing layer\n",
    "- new idea\n",
    "- new way to optimize\n",
    "- something new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09988e2d-3862-4c24-b79b-06a472be1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(filename, doc, model=\"llama3\"):\n",
    "    print(f\"file: {filename}\")\n",
    "    length = len(doc)\n",
    "    start = 0\n",
    "    embeddings = []\n",
    "    chunks = []\n",
    "    i = 0\n",
    "\n",
    "    while start < length:\n",
    "        end = min(start + token_size, length)\n",
    "        chunk = doc[start:end]\n",
    "        # print(f\"chunk: {chunk}\")\n",
    "        sumerize = ollama.chat(\n",
    "            model = model,\n",
    "            messages= [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"我要你的回答只能是 Array-List object\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"[]\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"很好，請針對下列 content:'{chunk}，產生一個 Array-List 包含屬於 content 的七個 關鍵字\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        print(f\"sumerize: {sumerize['message']}\")\n",
    "        chunk_to_embedding = sumerize['message']['content']\n",
    "        chunk_metadata = {\"type\":\"podcast\",\"name\":\"techporn\",\"title\":filename,\"documents\":chunk,\"ids\":i}\n",
    "        # print(f\"{chunk_to_embedding}\")\n",
    "        print(f\"metadata: {chunk_metadata}\")\n",
    "        uid = uuid.uuid4()\n",
    "        # print(f\"ids:{uid}\")\n",
    "        print(\"--\"*20)\n",
    "        \n",
    "        response = ollama.embeddings(model=model, prompt=chunk_to_embedding)\n",
    "        embedding = response[\"embedding\"]\n",
    "        chunks.append(chunk_to_embedding)\n",
    "        collection_add(str(uid), embedding, chunk, chunk_metadata)\n",
    "        start += shift_size  # Move start forward by the shift size\n",
    "        i += 1\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# def do_dataset_embedding(path2dataset, filenames, documents):\n",
    "#     for i, (f, d) in enumerate(zip(filenames, documents)):\n",
    "#         if sql_data.get(f, {}).get('status') != 'done':\n",
    "#             process_document(f, d)\n",
    "#             file_path = os.path.join(path2dataset, f)\n",
    "#             sql_data[f] = {'filename': f, 'path': file_path, 'status': 'done'}\n",
    "#         else:\n",
    "#             print(f\"...Skipping [DONE] - {f}\")\n",
    "\n",
    "do_dataset_embedding(path2dataset, filenames, documents)\n",
    "save_status(sql_file, sql_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe7811-02b1-4eea-b75e-5a503dc824bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
